# Python Outlier Detection (PyOD)

**Note: the project is currently under development without unit tests as of May 16th 2018. PyOD have been used in the following research projects:**
- Y. Zhao and M.K. Hryniewicki, "XGBOD: Improving Supervised Outlier Detection with Unsupervised Representation Learning," *International Joint Conference on Neural Networks*, IEEE, 2018.
- Y. Zhao and M.K. Hryniewicki, "DCSO: Dynamic Combination of Detector Scores for Outlier Ensembles," *ACM SIGKDD Workshop on Outlier Detection De-constructed*, 2018. Submitted, under review.

More anomaly detection related resources, e.g., books, papers and videos, can be found at [anomaly-detection-resources](https://github.com/yzhao062/anomaly-detection-resources)
<!-- TOC -->

- [Python Outlier Detection (PyOD)](#python-outlier-detection-pyod)
    - [Quick Introduction](#quick-introduction)
    - [API Cheatsheet](#api-cheatsheet)
    - [Quick Start for Outlier Detection](#quick-start-for-outlier-detection)
    - [Quick Start for Merging Outlier Scores](#quick-start-for-merging-outlier-scores)
    - [Reference](#reference)

<!-- /TOC -->

------------

### Quick Introduction
PyOD is a **Python-based toolkit** to identify outliers in data with (**mainly**) unsupervised and supervised approach. The toolkits consist of three major functionalities:
- Individual Algorithms  
  1. Local Outlier Factor (wrapped on sklearn implementation) [1]
  2. Isolation Forest (wrapped on sklearn implementation) [2]
  3. One-Class Support Vector Machines (wrapped on sklearn implementation) [3]
  4. KNN Outlier Detection 
  5. Average KNN Outlier Detection
  6. Median KNN Outlier Detection
  7. Global-Local Outlier Score From Hierarchies [4]
  8. Histogram-based Outlier Score (HBOS) [5]
  9. More to add

- Ensemble Framework (Outlier Score Combination Frameworks)
  1. Feature bagging
  2. Average of Maximum (AOM) [6]
  3. Maximum of Average (MOA) [6]
  4. Threshold Sum (Thresh) [6]
- Utility functions:
   1. scores_to_lables: converting raw outlier scores to binary labels
   2. precision_n_scores: one of the popular evaluation metrics for outlier mining (precision @ rank n)
  
Before using the toolkit, please be advised the purpose of the tool is for quick exploration. Using it as the final output should be understood with cautions. Fine-tunning may be needed to generate meaningful solution. I would recommend to use this as the first-step data exploration tool, and build your model/reuse the this model to get more accurate results.

------------
### API Cheatsheet
For all algorithms implemented/wrapped in PyOD, the similar API is forced for consistency.

- **fit()**: fitting the model with the training data
- **decision_function()**: return raw outlier scores for test data
- **predict()**: returning binary outlier labels of test data
- **predict_proba()**: returning outlier probability of test data (0 to 1)
- **predict_rank()**: returning outlier rank of test data (data outlyness rank in training data)

------------

### Quick Start for Outlier Detection
"example.py" is an example to demo the basic API of PyOD.
It first generate some sample data to run.  normal data is generated by a 2-d gaussian distribution, and outliers are generated by a 2-d uniform distribution.
````python
# percentage of outliers
contamination = 0.1
n_train = 1000
n_test = 500

# generate sample data
X_train, y_train, c_train, X_test, y_test, c_test = generate_data(n=n_train, contamination=contamination, n_test=n_test)
````
Then it initializes the classifier, fit the model, and make the prediction.
```python
# train a HBOS detector
clf = Hbos(contamination=0.1)
clf.fit(X_train)

# get the outlier score of the training data
y_train_pred = clf.y_pred
y_train_score = clf.decision_scores

# make the prediction on the test data
y_test_pred = clf.predict(X_test)
y_test_score = clf.decision_function(X_test)
```
The evaluation of the data is generated by:
```python
print('Precision@n on train data is', get_precn(y_train, y_train_score))
print('ROC on train data is', roc_auc_score(y_train, y_train_score))

print('Precision@n on test data is', get_precn(y_test, y_test_score))
print('ROC on test data is', roc_auc_score(y_test, y_test_score))
```
Here is a sample output:

	Precision@n on train data is 0.78 
	ROC on train data is 0.9360
	Precision@n on test data is 0.8780
	ROC on test data is 0.9872

    
To check the result of the classification visually:
![sample figure](https://github.com/yzhao062/Pyod/blob/master/figures/sample.png "sample figure")

### Quick Start for Merging Outlier Scores

"comb_example.py" is a quick demo for showing the API for combining multiple algorithms. Given we have n individual outlier detector, each of them generates an individual score for all samples. The task is to combine the outputs from these algorithms effectivelly.
**Key Step: conducting Z-score normalization on raw scores before the combination.**
Four combination mechanisms are shown in this demo:
1. Mean: use the mean value of all scores as the final output.
2. Max: use the max value of all scores as the final output.
3. Average of Maximum (AOM): first randomly split n detectors in to p groups. For each group, use the maximum within the group as the group output. Use the average of all group outputs as the final output.
4. Maximum of Average (MOA): similarly to AOM, the same grouping is introduced. However, we use the average of a group as the group output, and use maximum of all group outputs as the final output.
To better understand the merging techniques, refer to [6].

First initialize 20 KNN outlier detectors with different K (10 to 200), and get the outlier scores:
```python
    k_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140,
                  150, 160, 170, 180, 190, 200]
    n_clf = 20 # number of detectors
    train_scores = np.zeros([X_train.shape[0], n_clf])
    test_scores = np.zeros([X_test.shape[0], n_clf])
    for i in range(n_clf):
    	k = k_list[i]
    	clf = Knn(n_neighbors=k, method='largest')
    	clf.fit(X_train_norm)
    	train_scores[:, i] = clf.decision_scores.ravel()
    	test_scores[:, i] = clf.decision_function(X_test_norm).ravel()
```
Then the output codes are standardized into zero mean and unit std before combination.
```python
	scaler = StandardScaler().fit(train_scores)
	train_scores_norm = scaler.transform(train_scores)
	test_scores_norm = scaler.transform(test_scores)
```
Then four different combination algorithms are applied as described above:
```python
mean_result = np.mean(test_scores_norm, axis=1)
max_result = np.max(test_scores_norm, axis=1)
aom_result = aom(test_scores_norm, 5, 20) # 5 groups
moa_result = aom(test_scores_norm, 5, 20) # 5 groups
```
Finally, all four combination methods are evaluated with 20 iterations:

	summary
	mean roc: 0.920235078495
	max roc: 0.920535757004
	aom roc: 0.925942155568
	moa roc: 0.925871697126
	
### Reference
[1] Breunig, M.M., Kriegel, H.P., Ng, R.T. and Sander, J., 2000, May. LOF: identifying density-based local outliers. In *ACM sigmod record*, pp. 93-104. ACM.

[2] Liu, F.T., Ting, K.M. and Zhou, Z.H., 2008, December. Isolation forest. In *ICDM'08*, pp. 413-422. IEEE.

[3] Ma, J. and Perkins, S., 2003, July. Time-series novelty detection using one-class support vector machines. In *IJCNN' 03*, pp. 1741-1745. IEEE.

[4] Campello, R.J., Moulavi, D., Zimek, A. and Sander, J., 2015. Hierarchical density estimates for data clustering, visualization, and outlier detection. *ACM Transactions on Knowledge Discovery from Data (TKDD)*, 10(1), pp.5.

[5] Goldstein, M. and Dengel, A., 2012. Histogram-based outlier score (hbos): A fast unsupervised anomaly detection algorithm. In *KI-2012: Poster and Demo Track*, pp.59-63.

[6] Aggarwal, C.C. and Sathe, S., 2015. Theoretical foundations and algorithms for outlier ensembles.*ACM SIGKDD Explorations Newsletter*, 17(1), pp.24-47.

