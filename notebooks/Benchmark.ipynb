{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark of various outlier detection models\n",
    "\n",
    "**[PyOD](https://github.com/yzhao062/Pyod)** is a comprehensive **Python toolkit** to **identify outlying objects** in \n",
    "multivariate data with both unsupervised and supervised approaches.\n",
    "\n",
    "The following models are used for comparison:\n",
    "\n",
    "  1. Linear Models for Outlier Detection:\n",
    "     1. **PCA: Principal Component Analysis** use the sum of\n",
    "       weighted projected distances to the eigenvector hyperplane \n",
    "       as the outlier outlier scores) [10]\n",
    "     2. **One-Class Support Vector Machines** [3]\n",
    "     \n",
    "  2. Proximity-Based Outlier Detection Models:\n",
    "     1. **LOF: Local Outlier Factor** [1]\n",
    "     2. **kNN: k Nearest Neighbors** (use the distance to the kth nearest \n",
    "     neighbor as the outlier score)\n",
    "     3. **Average kNN** Outlier Detection (use the average distance to k \n",
    "     nearest neighbors as the outlier score)\n",
    "     4. **Median kNN** Outlier Detection (use the median distance to k nearest \n",
    "     neighbors as the outlier score)\n",
    "     5. **HBOS: Histogram-based Outlier Score** [5]\n",
    "     \n",
    "  3. Probabilistic Models for Outlier Detection:\n",
    "     1. **ABOD: Angle-Based Outlier Detection** [7]\n",
    "     2. **FastABOD: Fast Angle-Based Outlier Detection using approximation** [7]\n",
    "  \n",
    "  4. Outlier Ensembles and Combination Frameworks\n",
    "     1. **Isolation Forest** [2]\n",
    "     2. **Feature Bagging** [9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# temporary solution for relative imports in case pyod is not installed\n",
    "# if pyod is installed, no need to use the following line\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.iforest import IForest\n",
    "\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.utils.data import evaluate_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing annthyroid.mat\n",
      "Processing arrhythmia.mat\n",
      "Processing breastw.mat\n",
      "Processing cardio.mat\n",
      "Processing glass.mat\n",
      "Processing ionosphere.mat\n",
      "Processing letter.mat\n",
      "Processing lympho.mat\n",
      "Processing mammography.mat\n",
      "Processing mnist.mat\n",
      "Processing musk.mat\n",
      "Processing optdigits.mat\n",
      "Processing pendigits.mat\n",
      "Processing pima.mat\n",
      "Processing satellite.mat\n",
      "Processing satimage-2.mat\n",
      "Processing shuttle.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yzhao066\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\yzhao066\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int16 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing thyroid.mat\n",
      "Processing vertebral.mat\n",
      "Processing vowels.mat\n",
      "Processing wbc.mat\n"
     ]
    }
   ],
   "source": [
    "# Define data file and read X and y\n",
    "# Generate some data if the source data is missing\n",
    "\n",
    "mat_file_list = ['annthyroid.mat',\n",
    "                 'arrhythmia.mat',\n",
    "                 'breastw.mat',\n",
    "                 'cardio.mat',\n",
    "                 'glass.mat',\n",
    "                 'ionosphere.mat',\n",
    "                 'letter.mat',\n",
    "                 'lympho.mat',\n",
    "                 'mammography.mat',\n",
    "                 'mnist.mat',\n",
    "                 'musk.mat',\n",
    "                 'optdigits.mat',\n",
    "                 'pendigits.mat',\n",
    "                 'pima.mat',\n",
    "                 'satellite.mat',\n",
    "                 'satimage-2.mat',\n",
    "                 'shuttle.mat',\n",
    "                 'thyroid.mat',\n",
    "                 'vertebral.mat',\n",
    "                 'vowels.mat',\n",
    "                 'wbc.mat']\n",
    "\n",
    "\n",
    "for mat_file in mat_file_list:\n",
    "    print(\"Processing\", mat_file)\n",
    "    mat = loadmat(os.path.join('data', mat_file))\n",
    "\n",
    "    X = mat['X']\n",
    "    y = mat['y'].ravel()\n",
    "\n",
    "    # 60% data for training and 40% for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "    # standardizing data for processing\n",
    "    X_train_norm, X_test_norm = standardizer(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (226, 30) (226,)\n",
      "Test data: (152, 30) (152,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data:\", X_train.shape, y_train.shape)\n",
    "print(\"Test data:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing 20 kNN detectors\n",
      "Base detector 0 is fitted for prediction\n",
      "Base detector 1 is fitted for prediction\n",
      "Base detector 2 is fitted for prediction\n",
      "Base detector 3 is fitted for prediction\n",
      "Base detector 4 is fitted for prediction\n",
      "Base detector 5 is fitted for prediction\n",
      "Base detector 6 is fitted for prediction\n",
      "Base detector 7 is fitted for prediction\n",
      "Base detector 8 is fitted for prediction\n",
      "Base detector 9 is fitted for prediction\n",
      "Base detector 10 is fitted for prediction\n",
      "Base detector 11 is fitted for prediction\n",
      "Base detector 12 is fitted for prediction\n",
      "Base detector 13 is fitted for prediction\n",
      "Base detector 14 is fitted for prediction\n",
      "Base detector 15 is fitted for prediction\n",
      "Base detector 16 is fitted for prediction\n",
      "Base detector 17 is fitted for prediction\n",
      "Base detector 18 is fitted for prediction\n",
      "Base detector 19 is fitted for prediction\n"
     ]
    }
   ],
   "source": [
    "n_clf = 20  # number of base detectors\n",
    "\n",
    "# Initialize 20 base detectors for combination\n",
    "k_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140,\n",
    "          150, 160, 170, 180, 190, 200]\n",
    "\n",
    "train_scores = np.zeros([X_train.shape[0], n_clf])\n",
    "test_scores = np.zeros([X_test.shape[0], n_clf])\n",
    "\n",
    "print('Initializing {n_clf} kNN detectors'.format(n_clf=n_clf))\n",
    "\n",
    "for i in range(n_clf):\n",
    "    k = k_list[i]\n",
    "\n",
    "    clf = KNN(n_neighbors=k, method='largest')\n",
    "    clf.fit(X_train_norm)\n",
    "\n",
    "    train_scores[:, i] = clf.decision_scores_\n",
    "    test_scores[:, i] = clf.decision_function(X_test_norm)\n",
    "    print('Base detector %i is fitted for prediction' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision score matrix on training data (226, 20)\n",
      "Decision score matrix on test data (152, 20)\n"
     ]
    }
   ],
   "source": [
    "# Decision scores have to be normalized before combination\n",
    "train_scores_norm, test_scores_norm = standardizer(train_scores,\n",
    "                                                   test_scores)\n",
    "\n",
    "# Predicted scores from all base detectors on the test data is \n",
    "# stored in train_scores_norm and test_scores_norm\n",
    "print('Decision score matrix on training data', train_scores_norm.shape)\n",
    "print('Decision score matrix on test data', test_scores_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
